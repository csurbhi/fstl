From 617c4893182565645f1502c12e3eec288a33f0a4 Mon Sep 17 00:00:00 2001
From: Surbhi Palande <csurbhi@gmail.com>
Date: Tue, 28 Mar 2023 14:15:16 -0700
Subject: [PATCH] Code cleanup in GC

Verifying if number of valid blks matches with what gets read and
written by GC.

Signed-off-by: Surbhi Palande <csurbhi@gmail.com>
---
 lsdm.c | 240 +++++++++++++++++++++++++++++++++------------------------
 1 file changed, 139 insertions(+), 101 deletions(-)

diff --git a/lsdm.c b/lsdm.c
index 2cb796c..1b5f674 100644
--- a/lsdm.c
+++ b/lsdm.c
@@ -533,6 +533,7 @@ struct rev_extent * lsdm_rb_revmap_find(struct ctx *ctx, u64 pba, size_t len, u6
 			link = &(parent->rb_right);
 			continue;
 		}
+		//rev_e->pba == pba
 		return rev_e;
 	}
 	return higher_e;
@@ -1222,7 +1223,7 @@ static int read_gc_extents(struct ctx *ctx)
 	/* setup the bio for the first gc_extent */
 	list_for_each(pos, &ctx->gc_extents->list) {
 		gc_extent = list_entry(pos, struct gc_extents, list);
-		if ((gc_extent->e.len == 0) || gc_extent->e.pba > ctx->sb->max_pba) {
+		if ((gc_extent->e.len == 0) || ((gc_extent->e.pba + gc_extent->e.len) > ctx->sb->max_pba)) {
 			printk(KERN_ERR "\n %s lba: %llu, pba: %llu, len: %ld ", __func__, gc_extent->e.lba, gc_extent->e.pba, gc_extent->e.len);
 			BUG();
 		}
@@ -1257,6 +1258,8 @@ void move_gc_write_frontier(struct ctx *ctx, sector_t sectors_s8);
 
 static int setup_extent_bio_write(struct ctx *ctx, struct gc_extents *gc_extent);
 static void add_revmap_entry(struct ctx *, __le64, __le64, int);
+struct tm_page *add_tm_page_kv_store(struct ctx *, sector_t);
+int add_translation_entry(struct ctx *, struct page *, sector_t , sector_t , size_t ); 
 /* 
  * The extent that we are about to write will definitely fit into
  * the gc write frontier. No one is writing to the gc frontier
@@ -1271,31 +1274,56 @@ static int write_gc_extent(struct ctx *ctx, struct gc_extents *gc_extent)
 {
 	struct bio *bio;
 	struct gc_ctx *gc_ctx;
+	struct tm_page * tm_page = NULL;
 	sector_t s8;
 	int trials = 0;
 	int ret;
+	u64 lba, pba, len;
 
+	ret = refcount_read(&gc_extent->ref);
+	if (ret > 1) {
+		printk(KERN_ERR "\n waiting on refcount \n");
+		wait_on_refcount(ctx, &gc_extent->ref, &ctx->gc_ref_lock);
+	}
 	setup_extent_bio_write(ctx, gc_extent);
 	bio = gc_extent->bio;
 	bio->bi_iter.bi_sector = ctx->warm_gc_wf_pba;
 	gc_extent->e.pba = bio->bi_iter.bi_sector;
 	s8 = bio_sectors(bio);
 	
+	//printk(KERN_ERR "\n %s gc_extent: (lba: %d pba: %d len: %d), s8: %d max_pba: %llu", __func__, gc_extent->e.lba, gc_extent->e.pba, gc_extent->e.len, s8, ctx->sb->max_pba);
 	BUG_ON(gc_extent->e.len != s8);
 	BUG_ON(gc_extent->e.pba == 0);
 	BUG_ON(gc_extent->e.pba > ctx->sb->max_pba);
-	BUG_ON((gc_extent->e.pba + s8) > ctx->sb->max_pba);
 	BUG_ON(gc_extent->e.lba > ctx->sb->max_pba);
+	/* when e.pba is max_pba, len can at max be 1 */
+	if (gc_extent->e.pba == ctx->sb->max_pba) {
+		if (s8 != (1>>SECTOR_SHIFT)) {
+			BUG_ON(1);
+		}
+	}
 	bio->bi_status = BLK_STS_OK;
-	BUG_ON(gc_extent->e.pba > ctx->sb->max_pba);
 	submit_bio_wait(bio);
 	atomic_inc(&ctx->nr_gc_writes);
 	if (bio->bi_status != BLK_STS_OK) {
 		panic("GC writes failed! Perhaps a resource error");
 	}
-	//printk(KERN_ERR "\n %s lba: %llu pba: %llu , len: %llu e.len: %llu" , __func__, gc_extent->e.lba, gc_extent->e.pba, s8, gc_extent->e.len);
+	printk(KERN_ERR "\n %s lba: %llu pba: %llu , len: %llu e.len: %llu" , __func__, gc_extent->e.lba, gc_extent->e.pba, s8, gc_extent->e.len);
 	ret = lsdm_rb_update_range(ctx, gc_extent->e.lba, gc_extent->e.pba, gc_extent->e.len);
 	add_revmap_entry(ctx, gc_extent->e.lba, gc_extent->e.pba, gc_extent->e.len);
+	mutex_lock(&ctx->tm_kv_store_lock);
+	tm_page = add_tm_page_kv_store(ctx, gc_extent->e.lba);
+	if (!tm_page) {
+		mutex_unlock(&ctx->tm_kv_store_lock);
+		printk(KERN_ERR "%s NO memory! ", __func__);
+		return -ENOMEM;
+	}
+	tm_page->flag = NEEDS_FLUSH;
+	/* Call this under the kv store lock, else it will race with removal/flush code
+	 */
+	add_translation_entry(ctx, tm_page->page, gc_extent->e.lba, gc_extent->e.pba, gc_extent->e.len);
+	mutex_unlock(&ctx->tm_kv_store_lock);
+	move_gc_write_frontier(ctx, gc_extent->e.len);
 	return 0;
 }
 
@@ -1317,7 +1345,7 @@ static int setup_extent_bio_write(struct ctx *ctx, struct gc_extents *gc_extent)
 	//printk(KERN_ERR "\n %s gc_extent->e.len = %d ", __func__, s8);
 	bio_pages = (s8 >> SECTOR_SHIFT);
 	BUG_ON(bio_pages != gc_extent->nrpages);
-	//printk(KERN_ERR "\n 1) %s gc_extent->e.len (in sectors): %ld s8: %d", __func__, gc_extent->e.len, s8);
+	printk(KERN_ERR "\n 1) %s gc_extent->e.len (in sectors): %ld s8: %d bio_pages:%d", __func__, gc_extent->e.len, s8, bio_pages);
 
 	/* create a bio with "nr_pages" bio vectors, so that we can add nr_pages (nrpages is different)
 	 * individually to the bio vectors
@@ -1331,7 +1359,10 @@ static int setup_extent_bio_write(struct ctx *ctx, struct gc_extents *gc_extent)
 	/* bio_add_page sets the bi_size for the bio */
 	for(i=0; i<bio_pages; i++) {
 		page = gc_extent->bio_pages[i];
-		BUG_ON(!page);
+		if (!page) {
+			printk(KERN_ERR "\n %s i: %d ", __func__, i);
+			BUG_ON(!page);
+		}
 		if (!bio_add_page(bio, page, PAGE_SIZE, 0)) {
 			printk(KERN_ERR "\n %s Could not add page to the bio ", __func__);
 			printk(KERN_ERR "bio->bi_vcnt: %d bio->bi_iter.bi_size: %d bi_max_vecs: %d \n", bio->bi_vcnt, bio->bi_iter.bi_size, bio->bi_max_vecs);
@@ -1428,7 +1459,7 @@ int complete_revmap_blk_flush(struct ctx * ctx, struct page *page)
  * will end up writing invalid blocks and loosing the
  * overwritten data
  */
-static int write_valid_gc_extents(struct ctx *ctx, u64 last_pba)
+static int write_valid_gc_extents(struct ctx *ctx, u64 last_pba_read)
 {
 	struct extent *e = NULL;
 	struct rev_extent *rev_e = NULL;
@@ -1442,14 +1473,26 @@ static int write_valid_gc_extents(struct ctx *ctx, u64 last_pba)
 	int count = 0, pagecount = 0;
 	int i, j;
 
+	
 	list_for_each_entry_safe(gc_extent, next_ptr, &ctx->gc_extents->list, list) {
 		/* Reverse map stores PBA as e->lba and LBA as e->pba
 		 * This was done for code reuse between map and revmap
 		 * Thus e->lba is actually the PBA
 		 */
 		gc_extent->bio = NULL;
-		rev_e = lsdm_rb_revmap_find(ctx, gc_extent->e.pba, gc_extent->e.len, last_pba);
-prep:
+		//printk(KERN_ERR "\n %s gc_extent::(lba: %llu, pba: %llu, len: %d) last_pba_read: %llu", __func__, gc_extent->e.lba, gc_extent->e.pba, gc_extent->e.len, last_pba_read);
+		down_write(&ctx->lsdm_rb_lock);
+		/* You cannot use rb_next here as pba ie the key, changes when we write to a new frontier,
+		 * so the node is removed and rewritten at a different place, so the next is the new next 
+		 */
+		rev_e = lsdm_rb_revmap_find(ctx, gc_extent->e.pba, gc_extent->e.len, last_pba_read);
+		if (!rev_e) {
+			/* snip all the gc_extents from this onwards */
+			list_for_each_entry_safe_from(gc_extent, temp_ptr, &ctx->gc_extents->list, list) {
+				free_gc_extent(ctx, gc_extent);
+			}
+			return 0;
+		}
 		if (!rev_e) {
 			/* snip all the gc_extents from this onwards */
 			list_for_each_entry_safe_from(gc_extent, temp_ptr, &ctx->gc_extents->list, list) {
@@ -1464,7 +1507,7 @@ prep:
 			free_gc_extent(ctx, gc_extent);
 			continue;
 		}
-		/* extents are partially snipped */
+		/* extents are partially snipped at the front*/
 		if (e->pba > gc_extent->e.pba) {
 			//printk(KERN_ERR "\n %s:%d extent is snipped! \n", __func__, __LINE__);
 			gc_extent->e.lba = e->lba;
@@ -1475,8 +1518,13 @@ prep:
 			pagecount = gc_extent->e.len >> SECTOR_SHIFT;
 			BUG_ON(!pagecount);
 			/* free the extra pages */
-			for(i=pagecount; i<gc_extent->nrpages; i++) {
+			for(j=pagecount, i=0; j<gc_extent->nrpages; i++, j++) {
+				/* free the front pages already read */
 				mempool_free(gc_extent->bio_pages[i], ctx->gc_page_pool);
+				/* copy from back to the front */
+				gc_extent->bio_pages[i] = gc_extent->bio_pages[j];
+				/* free the back */
+				gc_extent->bio_pages[j] = NULL;
 			}
 			gc_extent->nrpages = pagecount;
 		}
@@ -1488,14 +1536,15 @@ prep:
 		BUG_ON(!nr_sectors);
 		s8 = round_up(nr_sectors, NR_SECTORS_IN_BLK);
 		BUG_ON(nr_sectors != s8);
+		BUG_ON(ctx->free_sectors_in_gc_wf < NR_SECTORS_IN_BLK);
 		if (nr_sectors > ctx->free_sectors_in_gc_wf) {
 			BUG_ON(!ctx->free_sectors_in_gc_wf);
 			printk(KERN_ERR "\n %s nr_sectors: %llu, ctx->free_sectors_in_gc_wf: %llu gc_extent->nrpages: %d", __func__, nr_sectors, ctx->free_sectors_in_gc_wf, gc_extent->nrpages);
 			gc_extent->e.len = ctx->free_sectors_in_gc_wf;
-			len = nr_sectors - gc_extent->e.len;
+			len = nr_sectors - ctx->free_sectors_in_gc_wf;
 			newgc_extent = kmem_cache_alloc(ctx->gc_extents_cache, GFP_KERNEL);
 			if(!newgc_extent) {
-				printk(KERN_ERR "\n Could not allocate memory to gc_extent! ");
+				printk(KERN_ERR "\n Could not allocate memory to new gc_extent! ");
 				BUG();
 				return -1;
 			}
@@ -1523,28 +1572,16 @@ prep:
 			gc_extent->nrpages = j;
 			for(i=0; i<pagecount; i++, j++) {
 				newgc_extent->bio_pages[i] = gc_extent->bio_pages[j];
+				gc_extent->bio_pages[j] = NULL;
 			}
 			list_add(&newgc_extent->list, &gc_extent->list);
 			next_ptr = newgc_extent;
+			printk(KERN_ERR "\n %s gc_extent::len: %d newgc_extent::len: %d ", __func__, gc_extent->e.len, newgc_extent->e.len);
 		}
 		/* Now you verify all this after holding a lock */
-		/*
-		down_write(&ctx->lsdm_rb_lock);
-		rev_e = lsdm_rb_revmap_find(ctx, gc_extent->e.pba, gc_extent->e.len, last_pba);
-		if (!rev_e || (rev_e->pba > gc_extent->e.pba)) {
-			up_write(&ctx->lsdm_rb_lock);
-			goto prep;
-		}
-		len = gc_extent->e.len;
-		ret = refcount_read(&gc_extent->ref);
-		if (ret > 1) {
-			printk(KERN_ERR "\n waiting on refcount \n");
-			wait_on_refcount(ctx, &gc_extent->ref, &ctx->gc_ref_lock);
-		}
 		ret = write_gc_extent(ctx, gc_extent);
 		up_write(&ctx->lsdm_rb_lock);
-		*/
-		ret = 0;
+		flush_workqueue(ctx->tm_wq);
 		if (ret) {
 			/* write error! disk is in a read mode! we
 			 * cannot perform any further GC
@@ -1553,7 +1590,6 @@ prep:
 			BUG();
 			return -1;
 		}
-		move_gc_write_frontier(ctx, len);
 		free_gc_extent(ctx, gc_extent);
 		count++;
 	}
@@ -1612,8 +1648,9 @@ void print_memory_usage(struct ctx *ctx, char *action)
 	printk(KERN_ERR "\n %s : available memory: %llu mB", action, available);
 }
 
+int get_sit_ent_vblocks(struct ctx *ctx, int zonenr);
 
-int create_gc_extents(struct ctx *ctx, sector_t pba, sector_t last_pba)
+int create_gc_extents(struct ctx *ctx, int zonenr)
 {
 	sector_t diff;
 	struct extent *e = NULL;
@@ -1621,7 +1658,13 @@ int create_gc_extents(struct ctx *ctx, sector_t pba, sector_t last_pba)
 	struct extent_entry temp;
 	long total_len = 0, total_extents = 0;
 	int count;
+	sector_t pba, last_pba; 
+	int vblks;
+	struct rb_node *node;
 
+	pba = get_first_pba_for_zone(ctx, zonenr);
+	last_pba = get_last_pba_for_zone(ctx, zonenr);
+	vblks = get_sit_ent_vblocks(ctx, zonenr);
 	INIT_LIST_HEAD(&ctx->gc_extents->list);
 
 	//print_memory_usage(ctx, "Before GC");
@@ -1632,15 +1675,14 @@ int create_gc_extents(struct ctx *ctx, sector_t pba, sector_t last_pba)
 	 * not need to perform GC on this segment.
 	 */
 
-	//printk(KERN_ERR "\n %s first_pba: %llu last_pba: %llu", __func__, pba, last_pba);
-
+	printk(KERN_ERR "\n %s zonenr: %d first_pba: %llu last_pba: %llu #valid blks: %d", __func__, zonenr, pba, last_pba, vblks);
+	temp.pba = 0;
+	temp.lba = 0;
+	temp.len = 0;
+	rev_e = lsdm_rb_revmap_find(ctx, pba, 0, last_pba);
+	BUG_ON(NULL == rev_e);
 	while(pba <= last_pba) {
 		//printk(KERN_ERR "\n %s Looking for pba: %llu" , __func__, pba);
-		rev_e = lsdm_rb_revmap_find(ctx, pba, 0, last_pba);
-		if (NULL == rev_e) {
-			//printk(KERN_ERR "\n Found NULL! ");
-			break;
-		}
 		e = rev_e->ptr_to_tm;
 		//printk(KERN_ERR "\n %s Found e, LBA: %llu, PBA: %llu, len: %u", __func__, e->lba, e->pba, e->len);
 		
@@ -1682,13 +1724,19 @@ int create_gc_extents(struct ctx *ctx, sector_t pba, sector_t last_pba)
 		total_len = total_len + temp.len;
 		total_extents = total_extents + count;
 		pba = temp.pba + temp.len;
-		//printk(KERN_ERR "\n %s (lba: %llu, pba: %llu e->len: %ld) last_pba: %lld total_len: %lld", __func__, temp.lba, temp.pba, temp.len, last_pba, total_len);
+		//printk(KERN_ERR "\n %s (lba: %llu, pba: %llu e->len: %ld) last_pba: %lld total_len: %lld", __func__, temp.lba, temp.pba, temp.len, last_pba, total_len);a
+		node = rb_next(&rev_e->rb);
+		if (NULL == node) {
+			//printk(KERN_ERR "\n Found NULL! ");
+			break;
+		}
+		rev_e = container_of(node, struct rev_extent, rb);
 	}
-	printk(KERN_ERR "\n %s Total extents: %llu, total_len: %llu \n", __func__, total_extents, total_len);
+	printk(KERN_ERR "\n %s Total extents: %llu, total_len: %llu vblks: %d \n", __func__, total_extents, total_len, vblks);
+	BUG_ON(total_len < vblks);
 	return total_extents;
 }
 
-int get_sit_ent_vblocks(struct ctx *ctx, int zonenr);
 
 /*
  * TODO: write code for FG_GC
@@ -1735,7 +1783,7 @@ static int lsdm_gc(struct ctx *ctx, int gc_mode, int err_flag)
 		io_schedule();
 		return -1;
 	}
-	flush_workqueue(ctx->tm_wq);
+	//flush_workqueue(ctx->tm_wq);
 	printk(KERN_ERR "\n Running GC!! zone_to_clean: %u  mode: %s", zonenr, (gc_mode == FG_GC) ? "FG_GC" : "BG_GC");
 again:
 	if (!list_empty(&ctx->gc_extents->list)) {
@@ -1747,10 +1795,7 @@ again:
 		printk(KERN_ERR "\n %s ************ extent list is not empty, #extents: %d ! ", __func__, count);
 	}
 
-	pba = get_first_pba_for_zone(ctx, zonenr);
-	last_pba = get_last_pba_for_zone(ctx, zonenr);
-	
-	create_gc_extents(ctx, pba, last_pba);
+	create_gc_extents(ctx, zonenr);
 
 	//print_memory_usage(ctx, "After extents");
 	/* Wait here till the system becomes IO Idle, if system is iodle don't wait */
@@ -1772,11 +1817,9 @@ again:
 		goto complete;
 	}
 
-	/*
 	ret = read_gc_extents(ctx);
 	if (ret)
 		goto failed;
-	*/
 	print_memory_usage(ctx, "After read");
 
 	/* Wait here till the system becomes IO Idle, if system is
@@ -1820,7 +1863,6 @@ complete:
 			if (zonenr >= 0)
 				goto again;
 		}
-		complete_revmap_blk_flush(ctx, ctx->revmap_page);
                 gc_th->gc_wake = 0;
 		wake_up_all(&ctx->gc_th->fggc_wq);
 		io_schedule();
@@ -2534,10 +2576,13 @@ static void mark_zone_free(struct ctx *ctx , int zonenr)
 	bitnr = zonenr % BITS_IN_BYTE;
 	//printk(KERN_ERR "\n %s zonenr: %lu, bytenr: %d bitnr: %d bitmap[%d]: %d ", __func__, zonenr, bytenr, bitnr,  bytenr, bitmap[bytenr]);
 
-	if(unlikely(bytenr > ctx->bitmap_bytes)) {
+	if(unlikely(bytenr >= ctx->bitmap_bytes)) {
 		panic("bytenr: %d > bitmap_bytes: %d", bytenr, ctx->bitmap_bytes);
 	}
 
+	if(unlikely((bytenr == (ctx->bitmap_bytes-1)) && (bitnr > ctx->bitmap_bit))) {
+		panic("bytenr: %d, bitnr: %d > bitmap_bytes: %d, bitnr: %d", bytenr, bitnr, ctx->bitmap_bytes, ctx->bitmap_bit);
+	}
 
 	if (unlikely(NULL == bitmap)) {
 		panic("This is a ctx freezone bitmap bug!");
@@ -2573,7 +2618,7 @@ static void mark_zone_gc_candidate(struct ctx *ctx , int zonenr)
 	int bitnr = zonenr % BITS_IN_BYTE;
 
 	//printk(KERN_ERR "\n %s zonenr: %lu, bytenr: %d bitnr: %d bitmap[%d]: %d", __func__, zonenr, bytenr, bitnr,  bytenr, bitmap[bytenr]);
-
+	//
 	if ((bitmap[bytenr] & (1 << bitnr)) == (1<<bitnr)) {
 		/* This bit was 1 and hence already free*/
 		panic("\n Trying to free an already free zone! ");
@@ -2600,15 +2645,13 @@ again:
 	while(bitmap[bytenr] == allZeroes) {
 		/* All these zones are occupied */
 		bytenr = bytenr + 1;
-		if (bytenr == ctx->bitmap_bytes) {
-			break;
+		if (unlikely(bytenr == ctx->bitmap_bytes)) {
+		/* no freezones available */
+			printk(KERN_ERR "\n No free zone available, disk is full! \n");
+			return -1;
 		}
 	}
 	
-	if (bytenr == ctx->bitmap_bytes) {
-		/* no freezones available */
-		return -1;
-	}
 	/* We have the bytenr from where to return the freezone */
 	bitnr = 0;
 	while (1) {
@@ -2617,13 +2660,17 @@ again:
 			break;
 		}
 		bitnr = bitnr + 1;
+		if(unlikely((bytenr == (ctx->bitmap_bytes-1)) && (bitnr == ctx->bitmap_bit))) {
+			printk(KERN_ERR "\n 2) No free zone available, disk is full! \n");
+			return -1;
+		}
 		if (bitnr == BITS_IN_BYTE) {
 			panic ("Wrong byte calculation!");
 		}
 	}
 	zonenr = (bytenr * BITS_IN_BYTE) + bitnr;
 	if (mark_zone_occupied(ctx, zonenr))
-		goto again;
+		BUG();
 
 	return zonenr;
 }
@@ -2659,8 +2706,8 @@ try_again:
 	/* get_next_freezone_nr() starts from 0. We need to adjust
 	 * the pba with that of the actual first PBA of data segment 0
 	 */
-	ctx->hot_wf_pba = ctx->sb->zone0_pba + (zone_nr << (ctx->sb->log_zone_size - ctx->sb->log_sector_size));
-	ctx->hot_wf_end = zone_end(ctx, ctx->hot_wf_pba);
+	ctx->hot_wf_pba = get_first_pba_for_zone(ctx, zone_nr);
+	ctx->hot_wf_end = get_last_pba_for_zone(ctx, zone_nr);
 
 	//printk(KERN_ERR "\n !!!!!!!!!!!!!!! get_new_zone():: zone0_pba: %u zone_nr: %d hot_wf_pba: %llu, wf_end: %llu \n", ctx->sb->zone0_pba, zone_nr, ctx->hot_wf_pba, ctx->hot_wf_end);
 	if (ctx->hot_wf_pba > ctx->hot_wf_end) {
@@ -2702,11 +2749,12 @@ again:
 	/* get_next_freezone_nr() starts from 0. We need to adjust
 	 * the pba with that of the actual first PBA of data segment 0
 	 */
-	ctx->warm_gc_wf_pba = ctx->sb->zone0_pba + (zone_nr << (ctx->sb->log_zone_size - ctx->sb->log_sector_size));
-	ctx->warm_gc_wf_end = zone_end(ctx, ctx->warm_gc_wf_pba);
+	ctx->warm_gc_wf_pba = get_first_pba_for_zone(ctx, zone_nr);
+	ctx->warm_gc_wf_end = get_last_pba_for_zone(ctx, zone_nr);
 	if (ctx->warm_gc_wf_pba > ctx->warm_gc_wf_end) {
 		panic("wf > wf_end!!, nr_free_sectors: %llu", ctx->free_sectors_in_wf );
 	}
+	BUG_ON(ctx->warm_gc_wf_end > ctx->sb->max_pba);
 	ctx->free_sectors_in_gc_wf = ctx->warm_gc_wf_end - ctx->warm_gc_wf_pba + 1;
 	add_ckpt_new_gc_wf(ctx, ctx->warm_gc_wf_pba);
 	printk("\n %s zone0_pba: %llu zone_nr: %d warm_gc_wf_pba: %llu, gc_wf_end: %llu", __func__,  ctx->sb->zone0_pba, zone_nr, ctx->warm_gc_wf_pba, ctx->warm_gc_wf_end);
@@ -3090,6 +3138,7 @@ void move_gc_write_frontier(struct ctx *ctx, sector_t s8)
 
 	BUG_ON(!s8);
 	
+	printk(KERN_ERR "\n %s ctx->warm_gc_wf: %llu ctx->free_sectors_in_gc_wf: %llu \n", __func__, ctx->warm_gc_wf_pba, ctx->free_sectors_in_gc_wf);
 	ctx->warm_gc_wf_pba = ctx->warm_gc_wf_pba + s8;
 	ctx->free_sectors_in_gc_wf = ctx->free_sectors_in_gc_wf - s8;
 	if (ctx->free_sectors_in_gc_wf < NR_SECTORS_IN_BLK) {
@@ -3256,7 +3305,7 @@ void sit_ent_vblocks_decr(struct ctx *ctx, sector_t pba)
 		update_gc_tree(ctx, zonenr, ptr->vblocks, ptr->mtime, __func__);
 		if (!ptr->vblocks) {
 			int ret = 0;
-			//printk(KERN_ERR "\n %s Freeing zone: %llu \n", __func__, zonenr);
+			printk(KERN_ERR "\n %s Freeing zone: %llu \n", __func__, zonenr);
 			mark_zone_free(ctx , zonenr);
 			
 		}
@@ -3352,7 +3401,6 @@ void sit_ent_add_mtime(struct ctx *ctx, sector_t pba)
 }
 
 struct tm_page * search_tm_kv_store(struct ctx *ctx, u64 blknr, struct rb_node **parent);
-struct tm_page *add_tm_page_kv_store(struct ctx *ctx, u64 lba);
 /*
  * If this length cannot be accomodated in this page
  * search and add another page for this next
@@ -3362,7 +3410,7 @@ struct tm_page *add_tm_page_kv_store(struct ctx *ctx, u64 lba);
  *
  * Depending on the location within a page, add the lba.
  */
-int add_translation_entry(struct ctx * ctx, struct page *page, __le64 lba, __le64 pba, int len) 
+int add_translation_entry(struct ctx * ctx, struct page *page, sector_t lba, sector_t pba, size_t len) 
 {
 	struct tm_entry * ptr;
 	int index, i, zonenr = 0;
@@ -3373,6 +3421,7 @@ int add_translation_entry(struct ctx * ctx, struct page *page, __le64 lba, __le6
 	BUG_ON(nrblks == 0);
 	BUG_ON(pba == 0);
 	ptr = (struct tm_entry *) page_address(page);
+	/* Do not modify the lba, we will need it later */
 	index = (lba >> SECTOR_SHIFT);
 	index = index  %  TM_ENTRIES_BLK;
 	ptr = ptr + index;
@@ -3398,8 +3447,6 @@ int add_translation_entry(struct ctx * ctx, struct page *page, __le64 lba, __le6
 			sit_ent_vblocks_decr(ctx, ptr->pba);
 		}
 		ptr->pba = pba;
-		zonenr = get_zone_nr(ctx, ptr->pba);
-		//printk(KERN_ERR "\n 2. %s lba: %llu, ptr->pba: %llu, new zone: %d ", __func__, lba, ptr->pba, zonenr);
 		sit_ent_vblocks_incr(ctx, pba);
 		pba = pba + NR_SECTORS_IN_BLK;
 		lba = lba + NR_SECTORS_IN_BLK;
@@ -3410,7 +3457,6 @@ int add_translation_entry(struct ctx * ctx, struct page *page, __le64 lba, __le6
 		} 
 		tm_page = add_tm_page_kv_store(ctx, lba);
 		if (!tm_page) {
-			mutex_unlock(&ctx->tm_kv_store_lock);
 			printk(KERN_ERR "%s NO memory! ", __func__);
 			return -ENOMEM;
 		}
@@ -3952,7 +3998,7 @@ int read_extents_from_block(struct ctx * ctx, struct tm_entry *entry, u64 lba);
  * Here blknr: is the relative TM block number that holds the entry
  * against this LBA. 
  */
-struct tm_page *add_tm_page_kv_store(struct ctx *ctx, u64 lba)
+struct tm_page *add_tm_page_kv_store(struct ctx *ctx, sector_t lba)
 {
 	struct rb_root *root = &ctx->tm_rb_root;
 	struct rb_node *parent = NULL, **link = &root->rb_node;
@@ -4197,23 +4243,12 @@ void wait_on_refcount(struct ctx *ctx, refcount_t *ref, spinlock_t *lock)
 	spin_unlock(lock);
 }
 
-void process_tm_entries(struct work_struct * w)
-{
-	struct revmap_bioctx * revmap_bio_ctx = container_of(w, struct revmap_bioctx, process_tm_work);
-	struct page *page = revmap_bio_ctx->page;
-	struct ctx * ctx = revmap_bio_ctx->ctx;
-
-	add_block_based_translation(ctx, page, __func__);
-	__free_pages(page, 0);
-	nrpages--;
-	kmem_cache_free(ctx->revmap_bioctx_cache, revmap_bio_ctx);
-}
-
 void revmap_blk_flushed(struct bio *bio)
 {
-	struct revmap_bioctx *revmap_bio_ctx = bio->bi_private;
-	struct ctx *ctx = revmap_bio_ctx->ctx;
+	struct ctx *ctx = bio->bi_private;
+	bio_free_pages(bio);
 	bio_put(bio);
+	nrpages--;
 	//mark_revmap_bit(ctx, revmap_bio_ctx->revmap_pba);
 	atomic_dec(&ctx->nr_revmap_flushes);
 	wake_up(&ctx->rev_blk_flushq);
@@ -4238,38 +4273,26 @@ int flush_revmap_block_disk(struct ctx * ctx, struct page *page, sector_t revmap
 
 	BUG_ON(!page);
 
-	revmap_bio_ctx = kmem_cache_alloc(ctx->revmap_bioctx_cache, GFP_KERNEL);
-	if (!revmap_bio_ctx) {
-		return -ENOMEM;
-	}
-
 	bio = bio_alloc(ctx->dev->bdev, 1, REQ_OP_WRITE, GFP_KERNEL);
 	if (!bio) {
-		kmem_cache_free(ctx->revmap_bioctx_cache, revmap_bio_ctx);
 		return -ENOMEM;
 	}
 	
 	if( PAGE_SIZE > bio_add_page(bio, page, PAGE_SIZE, 0)) {
 		bio_put(bio);
-		kmem_cache_free(ctx->revmap_bioctx_cache, revmap_bio_ctx);
 		return -EFAULT;
 	}
 
-	revmap_bio_ctx->ctx = ctx;
-	revmap_bio_ctx->page = page;
-
 	bio->bi_iter.bi_sector = revmap_pba;
 	//printk(KERN_ERR "%s Flushing revmap blk at pba:%llu ctx->revmap_pba: %llu", __func__, bio->bi_iter.bi_sector, ctx->revmap_pba);
 	bio->bi_end_io = revmap_blk_flushed;
+	bio->bi_private = ctx;
 	bio_set_op_attrs(bio, REQ_OP_WRITE, 0);
 	bio_set_dev(bio, ctx->dev->bdev);
-	bio->bi_private = revmap_bio_ctx;
 	atomic_inc(&ctx->nr_revmap_flushes);
 	BUG_ON(ctx->revmap_pba > ctx->sb->max_pba);
 	submit_bio(bio);
 	//printk(KERN_ERR "\n flushing revmap at pba: %llu page: %p", bio->bi_iter.bi_sector, page_address(page));
-	INIT_WORK(&revmap_bio_ctx->process_tm_work, process_tm_entries);
-	queue_work(ctx->tm_wq, &revmap_bio_ctx->process_tm_work);
 	return 0;
 }
 
@@ -4558,11 +4581,24 @@ void sub_write_done(struct work_struct * w)
 	trace_printk("\n %s Done !! lba: %llu, pba: %llu, len: %u kref: %d \n", __func__, lba, pba, len, kref_read(&bioctx->ref));
 	kmem_cache_free(ctx->subbio_ctx_cache, subbioctx);
 
-	//down_write(&ctx->lsdm_rev_lock);
+	down_write(&ctx->lsdm_rev_lock);
 	/*------------------------------- */
-	//add_revmap_entry(ctx, lba, pba, len);
+	add_revmap_entry(ctx, lba, pba, len);
 	/*-------------------------------*/
-	//up_write(&ctx->lsdm_rev_lock);
+	up_write(&ctx->lsdm_rev_lock);
+
+	mutex_lock(&ctx->tm_kv_store_lock);
+	tm_page = add_tm_page_kv_store(ctx, lba);
+	if (!tm_page) {
+		mutex_unlock(&ctx->tm_kv_store_lock);
+		printk(KERN_ERR "%s NO memory! ", __func__);
+		BUG();
+	}
+	tm_page->flag = NEEDS_FLUSH;
+	/* Call this under the kv store lock, else it will race with removal/flush code
+	 */
+	add_translation_entry(ctx, tm_page->page, lba, pba, len);
+	mutex_unlock(&ctx->tm_kv_store_lock);
 	return;
 }
 
@@ -4675,7 +4711,7 @@ int lsdm_write_checks(struct ctx *ctx, struct bio *bio)
 		goto fail;
 	}
 	if (ctx->nr_freezones <= ctx->lower_watermark) {
-		printk(KERN_ERR "\n 1. ctx->nr_freezones: %d, ctx->lower_watermark: %d. Starting GC.....\n", ctx->nr_freezones, ctx->lower_watermark);
+		//printk(KERN_ERR "\n 1. ctx->nr_freezones: %d, ctx->lower_watermark: %d. Starting GC.....\n", ctx->nr_freezones, ctx->lower_watermark);
 		DEFINE_WAIT(wait);
 		prepare_to_wait(&ctx->gc_th->fggc_wq, &wait,
 				TASK_UNINTERRUPTIBLE);
@@ -5960,9 +5996,11 @@ int read_metadata(struct ctx * ctx)
 	*/
 	ctx->nr_freezones = 0;
 	ctx->bitmap_bytes = sb2->zone_count_main /BITS_IN_BYTE;
-	if (sb2->zone_count_main % BITS_IN_BYTE)
+	if (sb2->zone_count_main % BITS_IN_BYTE) {
 		ctx->bitmap_bytes = ctx->bitmap_bytes + 1;
-	printk(KERN_ERR "\n %s Nr of zones in main are: %llu, bitmap_bytes: %d", __func__, sb2->zone_count_main, ctx->bitmap_bytes);
+		ctx->bitmap_bit = (sb2->zone_count_main % BITS_IN_BYTE);
+	}
+	printk(KERN_ERR "\n %s Nr of zones in main are: %llu, bitmap_bytes: %d, bitmap_bit: %d ", __func__, sb2->zone_count_main, ctx->bitmap_bytes, ctx->bitmap_bit);
 	if (sb2->zone_count_main % BITS_IN_BYTE > 0)
 	ctx->bitmap_bytes += 1;
 	ctx->nr_freezones = 0;
-- 
2.34.1

